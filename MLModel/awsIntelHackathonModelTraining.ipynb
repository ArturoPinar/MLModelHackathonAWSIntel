{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS y Intel Hackathon: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments==0.1.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch version needs to be the same in both the notebook instance and the training job container\n",
    "# https://github.com/pytorch/pytorch/issues/25214\n",
    "!{sys.executable} -m pip install torch==1.1.0\n",
    "!{sys.executable} -m pip install torchvision==0.3.0\n",
    "!{sys.executable} -m pip install pillow==6.2.2\n",
    "!{sys.executable} -m pip install --upgrade sagemaker\n",
    "!{sys.executable} -m pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import set_matplotlib_formats, display\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "set_matplotlib_formats(\"retina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/kc4xt9fhkrdnwjo/dataset_reduced.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instancias con menos HW puede ser usada tambien la version reducida del dataset, con aproximadamente 200 imagenes por clase. Para ello ejecutar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/lu3ftca48y92x1q/final_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3 as zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sess = sm_sess.boto_session\n",
    "sm = sm_sess.sagemaker_client\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "bucket = \"sagemaker-hackathon-demo-{}-{}\".format(sess.region_name, account_id)\n",
    "prefix = \"hackathon\"\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client(\"s3\").create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        sess.client(\"s3\").create_bucket(\n",
    "            Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": sess.region_name}\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", region_name = sess.region_name)\n",
    "\n",
    "inputs = None\n",
    "\n",
    "try:\n",
    "#     inputs = s3_resource.meta.client.upload_file(\"./dataset_reduced.zip\", bucket, \"hackathon/dataset_reduced.zip\")\n",
    "#     print(\"inputs\", inputs)\n",
    "    \n",
    "    inputs = sagemaker.Session().upload_data(path=\"./dataset_reduced.zip\", bucket=bucket, key_prefix=prefix)\n",
    "    print(\"input spec: {}\".format(inputs))\n",
    "except Exception as exp:\n",
    "    print(\"exp: \", exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().download_file(bucket,\"dataset_reduced.zip\",'./dataset_reduced_demo.zip')\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.download_file(\n",
    "    bucket,\n",
    "    \"hackathon/dataset_reduced.zip\",\n",
    "    \"dataset_reduced_2.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -uo dataset_reduced.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./final_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform= {\n",
    "    'dataset':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((505, 505)), # Mean size of images is 505\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'training':\n",
    "    transforms.Compose([\n",
    "         transforms.RandomResizedCrop(size=224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "    ]),\n",
    "    'validation': \n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "    ]),\n",
    "    'testing': \n",
    "    transforms.Compose([\n",
    "        transforms.Resize((505, 505)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root_path + \"train/\", transform=img_transform['dataset'])\n",
    "print('#{} train images loaded succesfully!'.format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(root_path + \"GT/\", transform=img_transform['dataset'])\n",
    "print('#{} test images loaded succesfully!'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    n = random.randrange(len(train_dataset))\n",
    "    print(\"n\", n)\n",
    "\n",
    "    print(\"Label = \", train_dataset[n][1] + 1, \":\")\n",
    "    display(transforms.ToPILImage()(train_dataset[n][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    n = random.randrange(len(test_dataset))\n",
    "    print(\"n\", n)\n",
    "\n",
    "    print(\"Label = \", test_dataset[n][1] + 1, \":\")\n",
    "    display(transforms.ToPILImage()(test_dataset[n][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_SIZE = .1\n",
    "n_val = round(SPLIT_SIZE * len(train_dataset))\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [len(train_dataset)-n_val, n_val])\n",
    "\n",
    "print('Spliting train images to #{} training samples and #{} samples for validation'.format(len(train_set), len(val_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We download the MNIST hand written digits dataset, and then apply transformation on each of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the images in the dataset, we upload it to s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets track the parameters from the data pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters(\n",
    "        {\n",
    "            \"normalization_mean\": 0.1307,\n",
    "            \"normalization_std\": 0.3081,\n",
    "        }\n",
    "    )\n",
    "#     # we can log the s3 uri to the dataset we just uploaded\n",
    "#     tracker.log_input(name=\"mnist-dataset\", media_type=\"s3/uri\", value=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Set up the Experiment\n",
    "\n",
    "Create an experiment to track all the model training iterations. Experiments are a great way to organize your data science work. You can create experiments to organize all your model development work for : [1] a business use case you are addressing (e.g. create experiment named “customer churn prediction”), or [2] a data science team that owns the experiment (e.g. create experiment named “marketing analytics experiment”), or [3] a specific data science and ML project. Think of it as a “folder” for organizing your “files”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_experiment = Experiment.create(\n",
    "    experiment_name=f\"hackathon-model-experiment-{int(time.time())}\",\n",
    "    description=\"Classification of trash in images into containers\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "print(model_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Track Experiment\n",
    "### Now create a Trial for each training run to track the it's inputs, parameters, and metrics.\n",
    "While training the CNN model on SageMaker, we will experiment with several values for the number of hidden channel in the model. We will create a Trial to track each training job run. We will also create a TrialComponent from the tracker we created before, and add to the Trial. This will enrich the Trial with the parameters we captured from the data pre-processing stage.\n",
    "\n",
    "Note the execution of the following code takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channel_trial_name_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the following training jobs asynchronously, you may need to increase your resource limit. Otherwise, you can run them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_trial_component = tracker.trial_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2, 5, 10, 20, 32]\n",
    "\n",
    "for i, num_hidden_channel in enumerate([2, 5]):\n",
    "    # create trial\n",
    "    trial_name = f\"cnn-training-job-{num_hidden_channel}-hidden-channels-{int(time.time())}\"\n",
    "    cnn_trial = Trial.create(\n",
    "        trial_name=trial_name,\n",
    "        experiment_name=model_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm,\n",
    "    )\n",
    "    hidden_channel_trial_name_map[num_hidden_channel] = trial_name\n",
    "\n",
    "    # associate the proprocessing trial component with the current trial\n",
    "    cnn_trial.add_trial_component(preprocessing_trial_component)\n",
    "\n",
    "    # all input configurations, parameters, and metrics specified in estimator\n",
    "    # definition are automatically tracked\n",
    "    estimator = PyTorch(\n",
    "        py_version=\"py3\",\n",
    "        entry_point=\"./model.py\",\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "        framework_version=\"1.1.0\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m4.xlarge\",\n",
    "        hyperparameters={\n",
    "            \"epochs\": 2,\n",
    "            \"backend\": \"gloo\",\n",
    "            \"hidden_channels\": num_hidden_channel,\n",
    "            \"dropout\": 0.2,\n",
    "            \"kernel_size\": 5,\n",
    "            \"optimizer\": \"sgd\",\n",
    "        },\n",
    "        metric_definitions=[\n",
    "            {\"Name\": \"train:loss\", \"Regex\": \"Train Loss: (.*?);\"},\n",
    "            {\"Name\": \"test:loss\", \"Regex\": \"Test Average loss: (.*?),\"},\n",
    "            {\"Name\": \"test:accuracy\", \"Regex\": \"Test Accuracy: (.*?)%;\"},\n",
    "        ],\n",
    "        enable_sagemaker_metrics=True,\n",
    "    )\n",
    "\n",
    "    cnn_training_job_name = \"cnn-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "    # Now associate the estimator with the Experiment and Trial\n",
    "    estimator.fit(\n",
    "        inputs={\"training\": inputs},\n",
    "        job_name=cnn_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cnn_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "\n",
    "    # give it a while before dispatching the next training job\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy endpoint for the best training-job / trial component\n",
    "\n",
    "Now we'll take the best (as sorted) and create an endpoint for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling best based on sort in the analytics/dataframe so first is best....\n",
    "best_trial_component_name = trial_component_analytics.dataframe().iloc[0][\"TrialComponentName\"]\n",
    "best_trial_component = TrialComponent.load(best_trial_component_name)\n",
    "\n",
    "model_data = best_trial_component.output_artifacts[\"SageMaker.ModelArtifact\"].value\n",
    "env = {\n",
    "    \"hidden_channels\": str(int(best_trial_component.parameters[\"hidden_channels\"])),\n",
    "    \"dropout\": str(best_trial_component.parameters[\"dropout\"]),\n",
    "    \"kernel_size\": str(int(best_trial_component.parameters[\"kernel_size\"])),\n",
    "}\n",
    "model = PyTorchModel(\n",
    "    model_data,\n",
    "    role,\n",
    "    \"./deploy-ei.py\",\n",
    "    py_version=\"py3\",\n",
    "    env=env,\n",
    "    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "    framework_version=\"1.1.0\",\n",
    "    name=best_trial_component.trial_component_name,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(instance_type=\"ml.m4.xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Once we're doing don't forget to clean up the endpoint to prevent unnecessary billing.\n",
    "\n",
    "> Trial components can exist independent of trials and experiments. You might want keep them if you plan on further exploration. If so, comment out tc.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
